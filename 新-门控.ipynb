{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de38618-76a0-401b-946f-f8d0c26b334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import find_objects\n",
    "import torchio as tio\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from configparser import ConfigParser\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from typing import Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "from dataset import ACDCDataset, PairwiseAugmentor\n",
    "\n",
    "# 配置参数\n",
    "CLASS_MAP = {'NOR':0, 'DCM':1, 'HCM':2, 'MINF':3, 'RV':4}\n",
    "TARGET_SHAPE = (200, 200, 80)\n",
    "TARGET_SPACING = 1.25  # mm\n",
    "AUG_FACTOR = 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b75098-8873-409f-975f-a9abf788f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps3D(nn.Module):\n",
    "    def __init__(self, in_channels=256, caps_dim=4, kernel_size=3, stride=2):\n",
    "        super().__init__()\n",
    "        self.caps_dim = caps_dim\n",
    "        self.out_caps = in_channels // caps_dim  # 主胶囊数量\n",
    "        self.conv = nn.Conv3d(in_channels, self.out_caps * caps_dim, \n",
    "                            kernel_size, stride=stride, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out = self.conv(x)  # [B, out_caps*caps_dim, D, H, W]\n",
    "        \n",
    "        # 重塑维度：分离胶囊维度和特征维度\n",
    "        out = out.view(batch_size, self.out_caps, self.caps_dim,\n",
    "                      out.size(-3), out.size(-2), out.size(-1))  # [B, out_caps, caps_dim, D, H, W]\n",
    "        \n",
    "        # 调整维度顺序并展平空间维度\n",
    "        out = out.permute(0, 3, 4, 5, 1, 2).contiguous()  # [B, D, H, W, out_caps, caps_dim]\n",
    "        spatial_flatten = out.size(1)*out.size(2)*out.size(3)  # p = D*H*W\n",
    "        return out.view(batch_size, spatial_flatten, self.out_caps, self.caps_dim)  # [B, p, i, m]\n",
    "\n",
    "class ConvCaps3D(nn.Module):\n",
    "    def __init__(self, in_caps, out_caps, in_dim, out_dim, num_routing=3):\n",
    "        super().__init__()\n",
    "        self.num_routing = num_routing\n",
    "        self.W = nn.Parameter(torch.Tensor(in_caps, out_caps, in_dim, out_dim))  # [i, j, m, n]\n",
    "        nn.init.orthogonal_(self.W)\n",
    "\n",
    "    def dynamic_routing(self, u):\n",
    "        \"\"\"动态路由机制\n",
    "        Args:\n",
    "            u: 输入胶囊 [B, p, i, m]\n",
    "        Returns:\n",
    "            v: 输出胶囊 [B, j, n]\n",
    "        \"\"\"\n",
    "        batch_size, p = u.size(0), u.size(1)\n",
    "        device = u.device\n",
    "        \n",
    "        # 计算预测向量 (公式1)\n",
    "        u_hat = torch.einsum('bpim,ijmn->bpijn', u, self.W)  # [B, p, j, n]\n",
    "        \n",
    "        # 初始化路由logits (公式2)\n",
    "        b = torch.zeros(batch_size, p, self.W.size(0), self.W.size(1)).to(device)  # [B, p, i, j]\n",
    "        \n",
    "        for _ in range(self.num_routing):\n",
    "            # 空间位置维度独立计算耦合系数\n",
    "            c = F.softmax(b, dim=-1)  # [B, p, i, j]\n",
    "            \n",
    "            # 加权求和 (公式3)\n",
    "            s = torch.einsum('bpij,bpijn->bjn', c, u_hat)  # [B, j, n]\n",
    "            \n",
    "            # 非线性压缩 (公式4)\n",
    "            v = self.squash(s)  # [B, j, n]\n",
    "            \n",
    "            # 路由协议更新 (仅在训练时更新)\n",
    "            if self.training and _ < self.num_routing-1:\n",
    "                delta_b = torch.einsum('bjn,bpijn->bpij', v, u_hat)\n",
    "                b = b + delta_b\n",
    "        \n",
    "        return v\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        norm = torch.norm(input_tensor, dim=-1, keepdim=True)  # [B, j, 1]\n",
    "        scale = norm**2 / (1 + norm**2)  # 缩放系数\n",
    "        return scale * input_tensor / (norm + 1e-8)  # [B, j, n]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dynamic_routing(x)\n",
    "\n",
    "class Caps3DNet(nn.Module):\n",
    "    \"\"\"改进后的三维胶囊网络\"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 图像特征提取\n",
    "        self.conv3d = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 256, kernel_size=9, stride=3),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Dropout3d(0.2)\n",
    "        )\n",
    "        \n",
    "        # 初级胶囊层 (输出[B, p=6*6*6=216, i=32, m=8])\n",
    "        self.primary_caps = PrimaryCaps3D(in_channels=256, caps_dim=4)\n",
    "        \n",
    "        # 卷积胶囊层 (i=32 -> j=64)\n",
    "        self.conv_caps1 = ConvCaps3D(\n",
    "            in_caps=32,\n",
    "            out_caps=64,\n",
    "            in_dim=8,\n",
    "            out_dim=16\n",
    "        )\n",
    "        \n",
    "        # 数字胶囊层 (j=64 -> num_classes)\n",
    "        self.digit_caps = ConvCaps3D(\n",
    "            in_caps=64,\n",
    "            out_caps=num_classes,\n",
    "            in_dim=4,\n",
    "            out_dim=8\n",
    "        )\n",
    "        \n",
    "        # EF特征分支\n",
    "        self.ef_fc = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Linear(num_classes*8 + 32, num_classes)\n",
    "\n",
    "    def forward(self, x, ef):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 图像特征处理\n",
    "        x = self.conv3d(x)  # [B, 256, 6,6,6]\n",
    "        x = self.primary_caps(x)  # [B, p=216, i=32, m=8]\n",
    "        \n",
    "        # 胶囊层处理\n",
    "        caps_output = self.digit_caps(x)  # [B, num_classes, 16]\n",
    "        \n",
    "        # 展平胶囊输出\n",
    "        img_features = caps_output.view(batch_size, -1)  # [B, num_classes*16]\n",
    "        \n",
    "        # EF特征处理\n",
    "        ef_features = self.ef_fc(ef.unsqueeze(1))  # [B, 32]\n",
    "        \n",
    "        # 特征融合\n",
    "        combined = torch.cat([img_features, ef_features], dim=1)  # [B, num_classes*16+32]\n",
    "        \n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31318e73-78d9-42e5-bcbe-f1568e337e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing results file found. Starting fresh.\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Early stopping at epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301103/863503844.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Accuracy: 66.67%\n",
      "\n",
      "Current 5-Fold CV Results: [0.6666666666666666]\n",
      "Average Accuracy: 66.67% (±0.00%)\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301103/863503844.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Accuracy: 53.33%\n",
      "\n",
      "Current 5-Fold CV Results: [0.6666666666666666, 0.5333333333333333]\n",
      "Average Accuracy: 60.00% (±6.67%)\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Early stopping at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301103/863503844.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Accuracy: 51.67%\n",
      "\n",
      "Current 5-Fold CV Results: [0.6666666666666666, 0.5333333333333333, 0.5166666666666667]\n",
      "Average Accuracy: 57.22% (±6.71%)\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Early stopping at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301103/863503844.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Accuracy: 58.33%\n",
      "\n",
      "Current 5-Fold CV Results: [0.6666666666666666, 0.5333333333333333, 0.5166666666666667, 0.5833333333333334]\n",
      "Average Accuracy: 57.50% (±5.83%)\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Early stopping at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301103/863503844.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Accuracy: 56.67%\n",
      "\n",
      "Current 5-Fold CV Results: [0.6666666666666666, 0.5333333333333333, 0.5166666666666667, 0.5833333333333334, 0.5666666666666667]\n",
      "Average Accuracy: 57.33% (±5.23%)\n",
      "\n",
      "=== Final Results ===\n",
      "5-Fold CV Results: [0.6666666666666666, 0.5333333333333333, 0.5166666666666667, 0.5833333333333334, 0.5666666666666667]\n",
      "Average Accuracy: 57.33% (±5.23%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import torch.multiprocessing as mp\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':  # 确保在主模块中设置\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "start_fold = 0  # 可修改为需要开始的折数 (0-4)\n",
    "results_file = '新-门控.json'\n",
    "CUSTOM_PREFIX = \"新-门控\"\n",
    "\n",
    "# 尝试加载已有的结果 - 添加空文件处理\n",
    "fold_results = []\n",
    "if os.path.exists(results_file):\n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            file_content = f.read().strip()\n",
    "            if file_content:  # 检查文件是否非空\n",
    "                fold_results = json.loads(file_content)\n",
    "                print(f\"Loaded existing results: {fold_results}\")\n",
    "            else:\n",
    "                print(\"Results file exists but is empty. Starting fresh.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Results file contains invalid JSON. Starting fresh.\")\n",
    "        fold_results = []\n",
    "else:\n",
    "    print(\"No existing results file found. Starting fresh.\")\n",
    "\n",
    "# 训练流程修改\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for inputs, labels, ef in loader:\n",
    "        inputs, labels, ef = inputs.to(device), labels.to(device), ef.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, ef)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())  # 使用extend代替append\n",
    "        all_labels.extend(labels.cpu().numpy()) # 转换为numpy数组\n",
    "    \n",
    "    return running_loss/len(loader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, ef in loader:\n",
    "            inputs, labels, ef = inputs.to(device), labels.to(device), ef.float().to(device)\n",
    "            \n",
    "            outputs = model(inputs, ef)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())   # 修改为extend\n",
    "            val_labels.extend(labels.cpu().numpy())  # 修改为extend\n",
    "    \n",
    "    return val_loss/len(loader.dataset), accuracy_score(val_labels, val_preds)\n",
    "\n",
    "\n",
    "# 五折交叉验证修改版\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_cases = [d for d in Path('心力衰竭/database/training').glob('patient*') if d.is_dir()] + \\\n",
    "            [d for d in Path('心力衰竭/database/testing').glob('patient*') if d.is_dir()]\n",
    "all_labels = []  # 存储每个病例的标签\n",
    "\n",
    "# 收集每个病例的标签\n",
    "for case in all_cases:\n",
    "    # 创建临时数据集实例（不需要变换）\n",
    "    _, label, _ = ACDCDataset([case], phase='train')[0]\n",
    "    all_labels.append(label)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(all_cases, all_labels)):\n",
    "    print(f\"\\n=== Fold {fold+1}/5 ===\")\n",
    "    \n",
    "    # 划分训练验证集和测试集\n",
    "    train_val_cases = [all_cases[i] for i in train_val_idx]\n",
    "    test_cases = [all_cases[i] for i in test_idx]\n",
    "    \n",
    "    # 从训练验证集中提取标签用于再分层\n",
    "    train_val_labels = [all_labels[i] for i in train_val_idx]\n",
    "    \n",
    "    # 在训练验证集内部进行分层划分 (75%训练, 25%验证)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "    for train_idx, val_idx in sss.split(train_val_cases, train_val_labels):\n",
    "        train_cases = [train_val_cases[i] for i in train_idx]\n",
    "        val_cases = [train_val_cases[i] for i in val_idx]\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = ACDCDataset(train_cases, phase='train')\n",
    "    val_dataset = ACDCDataset(val_cases, phase='val')    # 从训练集划分的验证集\n",
    "    test_dataset = ACDCDataset(test_cases, phase='val')  # 独立测试集\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    # 模型初始化（保持原有实现不变）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Caps3DNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5)\n",
    "    \n",
    "    # 初始化跟踪变量\n",
    "    best_acc = 0.0  # 只跟踪最佳准确率\n",
    "    best_loss = 10\n",
    "    best_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_best.pth\"\n",
    "    final_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_last.pth\"\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)  # 使用新验证集\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # 动态保存最佳模型（只保留最佳准确率版本）\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        \n",
    "        # 早停判断（基于验证损失）\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 10:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    fold_results.append(test_acc)  # 记录测试集准确率\n",
    "    print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2%}\")    \n",
    "    \n",
    "    # 确保最终模型保存（即使早停也保存最后达到的epoch）\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(fold_results, f)\n",
    "    print(f\"\\nCurrent 5-Fold CV Results: {fold_results}\")\n",
    "    print(f\"Average Accuracy: {np.mean(fold_results):.2%} (±{np.std(fold_results):.2%})\")\n",
    "\n",
    "# 输出结果（保持原有实现不变）\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"5-Fold CV Results: {final_results}\")\n",
    "print(f\"Average Accuracy: {np.mean(final_results):.2%} (±{np.std(final_results):.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
