{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575542e6-4cde-4377-9b76-14f381f02af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import find_objects\n",
    "import torchio as tio\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from configparser import ConfigParser\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from typing import Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "from Fdataset import ACDCDataset, PairwiseAugmentor\n",
    "\n",
    "# 配置参数\n",
    "CLASS_MAP = {'NOR':0, 'DCM':1, 'HCM':2, 'MINF':3, 'RV':4}\n",
    "TARGET_SHAPE = (200, 200, 80)\n",
    "TARGET_SPACING = 1.25  # mm\n",
    "AUG_FACTOR = 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a1ce5-f41b-459c-869a-9b71475c4171",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class ConvBlock3D(nn.Module):\n",
    "    \"\"\"3D卷积块（带SE注意力）\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, expansion=4, se_ratio=0.25):\n",
    "        super().__init__()\n",
    "        expanded = in_channels * expansion\n",
    "        self.conv1 = nn.Conv3d(in_channels, expanded, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(expanded)\n",
    "        \n",
    "        # 深度可分离卷积\n",
    "        self.depthwise = nn.Conv3d(\n",
    "            expanded, expanded, kernel_size, stride, \n",
    "            padding=kernel_size//2, groups=expanded, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm3d(expanded)\n",
    "        \n",
    "        # SE注意力\n",
    "        se_channels = max(1, int(expanded * se_ratio))\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Conv3d(expanded, se_channels, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv3d(se_channels, expanded, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(expanded, out_channels, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = self.act(self.bn1(self.conv1(x)))\n",
    "        x = self.act(self.bn2(self.depthwise(x)))\n",
    "        \n",
    "        # 应用SE\n",
    "        se = self.se(x)\n",
    "        x = x * se\n",
    "        \n",
    "        x = self.bn3(self.conv2(x))\n",
    "        return self.act(x + residual)\n",
    "\n",
    "class EfficientNetV2_3D(nn.Module):\n",
    "    \"\"\"3D版EfficientNetV2（简化架构）\"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=5):\n",
    "        super().__init__()\n",
    "        # 初始卷积层\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # MBConv模块序列\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock3D(32, 24, 3, stride=1, expansion=1),\n",
    "            ConvBlock3D(24, 48, 3, stride=2, expansion=4),\n",
    "            ConvBlock3D(48, 64, 3, stride=2, expansion=4),\n",
    "            ConvBlock3D(64, 128, 3, stride=2, expansion=4),\n",
    "            ConvBlock3D(128, 160, 3, stride=1, expansion=6),\n",
    "            ConvBlock3D(160, 256, 3, stride=2, expansion=6)\n",
    "        )\n",
    "        \n",
    "        # 头部\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv3d(256, 1280, 1, bias=False),\n",
    "            nn.BatchNorm3d(1280),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class PatchEmbedding3D(nn.Module):\n",
    "    \"\"\"3D图像分块嵌入\"\"\"\n",
    "    def __init__(self, patch_size=16, in_chans=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_chans, embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, C, D, H, W]\n",
    "        x = rearrange(x, 'b c d h w -> b (d h w) c')\n",
    "        return self.norm(x)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器层（3D）\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 自注意力\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = res + x\n",
    "        \n",
    "        # MLP\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        return res + x\n",
    "\n",
    "class VisionTransformer3D(nn.Module):\n",
    "    \"\"\"3D版Vision Transformer\"\"\"\n",
    "    def __init__(self, in_chans=1, num_classes=5, \n",
    "                 patch_size=16, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding3D(patch_size, in_chans, embed_dim)\n",
    "        \n",
    "        # 位置编码\n",
    "        num_patches = (200//patch_size) * (200//patch_size) * (80//patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer编码器\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerEncoder(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # 添加cls token\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.pos_embed\n",
    "        \n",
    "        # Transformer编码\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 分类头\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits1 = self.model1(x)\n",
    "        logits2 = self.model2(x)\n",
    "        \n",
    "        # 几何平均（概率空间）\n",
    "        prob1 = F.softmax(logits1, dim=-1)\n",
    "        prob2 = F.softmax(logits2, dim=-1)\n",
    "        geom_prob = torch.sqrt(prob1 * prob2 + 1e-9)\n",
    "        \n",
    "        # 转换回logits形式（对数概率）\n",
    "        return torch.log(geom_prob)  # CrossEntropyLoss需要logits输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f242d-0ab8-41a9-8217-c547eb71323a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import torch.multiprocessing as mp\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':  # 确保在主模块中设置\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "start_fold = 0  # 可修改为需要开始的折数 (0-4)\n",
    "results_file = '新-trans.json'\n",
    "CUSTOM_PREFIX = \"新-trans\"\n",
    "\n",
    "# 尝试加载已有的结果 - 添加空文件处理\n",
    "fold_results = []\n",
    "if os.path.exists(results_file):\n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            file_content = f.read().strip()\n",
    "            if file_content:  # 检查文件是否非空\n",
    "                fold_results = json.loads(file_content)\n",
    "                print(f\"Loaded existing results: {fold_results}\")\n",
    "            else:\n",
    "                print(\"Results file exists but is empty. Starting fresh.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Results file contains invalid JSON. Starting fresh.\")\n",
    "        fold_results = []\n",
    "else:\n",
    "    print(\"No existing results file found. Starting fresh.\")\n",
    "\n",
    "# 训练流程修改\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())  # 使用extend代替append\n",
    "        all_labels.extend(labels.cpu().numpy()) # 转换为numpy数组\n",
    "    \n",
    "    return running_loss/len(loader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())   # 修改为extend\n",
    "            val_labels.extend(labels.cpu().numpy())  # 修改为extend\n",
    "    \n",
    "    return val_loss/len(loader.dataset), accuracy_score(val_labels, val_preds)\n",
    "\n",
    "\n",
    "# 五折交叉验证修改版\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_cases = [d for d in Path('心力衰竭/database/training').glob('patient*') if d.is_dir()] + \\\n",
    "            [d for d in Path('心力衰竭/database/testing').glob('patient*') if d.is_dir()]\n",
    "all_labels = []  # 存储每个病例的标签\n",
    "\n",
    "# 收集每个病例的标签\n",
    "for case in all_cases:\n",
    "    # 创建临时数据集实例（不需要变换）\n",
    "    _, label = ACDCDataset([case], phase='train')[0]\n",
    "    all_labels.append(label)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(all_cases, all_labels)):\n",
    "    print(f\"\\n=== Fold {fold+1}/5 ===\")\n",
    "    \n",
    "    # 划分训练验证集和测试集\n",
    "    train_val_cases = [all_cases[i] for i in train_val_idx]\n",
    "    test_cases = [all_cases[i] for i in test_idx]\n",
    "    \n",
    "    # 从训练验证集中提取标签用于再分层\n",
    "    train_val_labels = [all_labels[i] for i in train_val_idx]\n",
    "    \n",
    "    # 在训练验证集内部进行分层划分 (75%训练, 25%验证)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "    for train_idx, val_idx in sss.split(train_val_cases, train_val_labels):\n",
    "        train_cases = [train_val_cases[i] for i in train_idx]\n",
    "        val_cases = [train_val_cases[i] for i in val_idx]\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = ACDCDataset(train_cases, phase='train')\n",
    "    val_dataset = ACDCDataset(val_cases, phase='val')    # 从训练集划分的验证集\n",
    "    test_dataset = ACDCDataset(test_cases, phase='val')  # 独立测试集\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    # 模型初始化（保持原有实现不变）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    effnet = EfficientNetV2_3D()\n",
    "    vit = VisionTransformer3D(patch_size=20, depth=6, embed_dim=512, num_heads=8)  # 简化ViT\n",
    "    model = EnsembleModel(effnet, vit).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5)\n",
    "    \n",
    "    # 初始化跟踪变量\n",
    "    best_acc = 0.0  # 只跟踪最佳准确率\n",
    "    best_loss = 10\n",
    "    best_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_best.pth\"\n",
    "    final_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_last.pth\"\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)  # 使用新验证集\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # 动态保存最佳模型（只保留最佳准确率版本）\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        \n",
    "        # 早停判断（基于验证损失）\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 10:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    fold_results.append(test_acc)  # 记录测试集准确率\n",
    "    print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2%}\")    \n",
    "    \n",
    "    # 确保最终模型保存（即使早停也保存最后达到的epoch）\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(fold_results, f)\n",
    "    print(f\"\\nCurrent 5-Fold CV Results: {fold_results}\")\n",
    "    print(f\"Average Accuracy: {np.mean(fold_results):.2%} (±{np.std(fold_results):.2%})\")\n",
    "\n",
    "# 输出结果（保持原有实现不变）\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"5-Fold CV Results: {final_results}\")\n",
    "print(f\"Average Accuracy: {np.mean(final_results):.2%} (±{np.std(final_results):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0cf3af-e7bf-42df-8b99-80fa57d53241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock3D(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction_ratio, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeatureExtractor3D(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(FeatureExtractor3D, self).__init__()\n",
    "        # 输入: (batch, 1, 80, 200, 200)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)  # 输出: (40, 100, 100)\n",
    "        )\n",
    "        self.se1 = SEBlock3D(64)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)  # 输出: (20, 50, 50)\n",
    "        )\n",
    "        self.se2 = SEBlock3D(32)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)  # 输出: (10, 25, 25)\n",
    "        )\n",
    "        \n",
    "        # 展平后的特征维度: 32 * 10 * 25 * 25 = 200,000\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.se1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.se2(x)\n",
    "        x = self.conv3(x)\n",
    "        return self.flatten(x)  # 输出: (batch, 200,000)\n",
    "\n",
    "\n",
    "# 模型1：多二分类器 (每个类别独立)\n",
    "class MultiBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=3):\n",
    "        super(MultiBinaryClassifier, self).__init__()\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([cls(x) for cls in self.classifiers], dim=1)\n",
    "\n",
    "# 模型2：标准多类分类器\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=3):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 模型3：贝叶斯分类器 (使用MC Dropout模拟不确定性)\n",
    "class BayesianClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=3):\n",
    "        super(BayesianClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if sample: x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if sample: x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class VS_BEAM_3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=5):\n",
    "        super(VS_BEAM_3D, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor3D(in_channels)\n",
    "        input_size = 200000  # 32 * 10 * 25 * 25\n",
    "        \n",
    "        self.model1 = MultiBinaryClassifier(input_size, num_classes)\n",
    "        self.model2 = MultiClassClassifier(input_size, num_classes)\n",
    "        self.model3 = BayesianClassifier(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 三个模型的预测\n",
    "        out1 = self.model1(features)  # [batch, num_classes]\n",
    "        out2 = self.model2(features)  # [batch, num_classes]\n",
    "        out3 = self.model3(features)  # [batch, num_classes]\n",
    "        \n",
    "        # 投票机制\n",
    "        combined = torch.stack([out1, out2, out3], dim=2)  # [batch, classes, 3]\n",
    "        votes = torch.argmax(combined, dim=1)  # [batch, 3]\n",
    "        final = torch.mode(votes, dim=1).values\n",
    "        \n",
    "        return {\n",
    "            'model1': out1,\n",
    "            'model2': out2,\n",
    "            'model3': out3,\n",
    "            'final': final\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8850ffe-7662-4cdd-83f4-57bd2c5e8d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing results file found. Starting fresh.\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197956/2078591851.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Accuracy: 55.00%\n",
      "\n",
      "Current 5-Fold CV Results: [0.55]\n",
      "Average Accuracy: 55.00% (±0.00%)\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197956/2078591851.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Accuracy: 50.00%\n",
      "\n",
      "Current 5-Fold CV Results: [0.55, 0.5]\n",
      "Average Accuracy: 52.50% (±2.50%)\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197956/2078591851.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Accuracy: 61.67%\n",
      "\n",
      "Current 5-Fold CV Results: [0.55, 0.5, 0.6166666666666667]\n",
      "Average Accuracy: 55.56% (±4.78%)\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197956/2078591851.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Accuracy: 58.33%\n",
      "\n",
      "Current 5-Fold CV Results: [0.55, 0.5, 0.6166666666666667, 0.5833333333333334]\n",
      "Average Accuracy: 56.25% (±4.31%)\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197956/2078591851.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Accuracy: 43.33%\n",
      "\n",
      "Current 5-Fold CV Results: [0.55, 0.5, 0.6166666666666667, 0.5833333333333334, 0.43333333333333335]\n",
      "Average Accuracy: 53.67% (±6.45%)\n",
      "\n",
      "=== Final Results ===\n",
      "5-Fold CV Results: [0.55, 0.5, 0.6166666666666667, 0.5833333333333334, 0.43333333333333335]\n",
      "Average Accuracy: 53.67% (±6.45%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import torch.multiprocessing as mp\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':  # 确保在主模块中设置\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "start_fold = 0  # 可修改为需要开始的折数 (0-4)\n",
    "results_file = '新-classi.json'\n",
    "CUSTOM_PREFIX = \"新-classi\"\n",
    "\n",
    "# 尝试加载已有的结果 - 添加空文件处理\n",
    "fold_results = []\n",
    "if os.path.exists(results_file):\n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            file_content = f.read().strip()\n",
    "            if file_content:  # 检查文件是否非空\n",
    "                fold_results = json.loads(file_content)\n",
    "                print(f\"Loaded existing results: {fold_results}\")\n",
    "            else:\n",
    "                print(\"Results file exists but is empty. Starting fresh.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Results file contains invalid JSON. Starting fresh.\")\n",
    "        fold_results = []\n",
    "else:\n",
    "    print(\"No existing results file found. Starting fresh.\")\n",
    "\n",
    "# 训练流程修改\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss1 = criterion(outputs['model1'], labels)\n",
    "        loss2 = criterion(outputs['model2'], labels)\n",
    "        loss3 = criterion(outputs['model3'], labels)\n",
    "        loss = loss1 + loss2 + loss3\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs['final']\n",
    "        all_preds.extend(preds.cpu().numpy())  # 使用extend代替append\n",
    "        all_labels.extend(labels.cpu().numpy()) # 转换为numpy数组\n",
    "    \n",
    "    return running_loss/len(loader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss1 = criterion(outputs['model1'], labels)\n",
    "            loss2 = criterion(outputs['model2'], labels)\n",
    "            loss3 = criterion(outputs['model3'], labels)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            preds = outputs['final']\n",
    "            val_preds.extend(preds.cpu().numpy())   # 修改为extend\n",
    "            val_labels.extend(labels.cpu().numpy())  # 修改为extend\n",
    "    \n",
    "    return val_loss/len(loader.dataset), accuracy_score(val_labels, val_preds)\n",
    "\n",
    "\n",
    "# 五折交叉验证修改版\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_cases = [d for d in Path('心力衰竭/database/training').glob('patient*') if d.is_dir()] + \\\n",
    "            [d for d in Path('心力衰竭/database/testing').glob('patient*') if d.is_dir()]\n",
    "all_labels = []  # 存储每个病例的标签\n",
    "\n",
    "# 收集每个病例的标签\n",
    "for case in all_cases:\n",
    "    # 创建临时数据集实例（不需要变换）\n",
    "    _, label = ACDCDataset([case], phase='train')[0]\n",
    "    all_labels.append(label)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(kf.split(all_cases, all_labels)):\n",
    "    print(f\"\\n=== Fold {fold+1}/5 ===\")\n",
    "    \n",
    "    # 划分训练验证集和测试集\n",
    "    train_val_cases = [all_cases[i] for i in train_val_idx]\n",
    "    test_cases = [all_cases[i] for i in test_idx]\n",
    "    \n",
    "    # 从训练验证集中提取标签用于再分层\n",
    "    train_val_labels = [all_labels[i] for i in train_val_idx]\n",
    "    \n",
    "    # 在训练验证集内部进行分层划分 (75%训练, 25%验证)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "    for train_idx, val_idx in sss.split(train_val_cases, train_val_labels):\n",
    "        train_cases = [train_val_cases[i] for i in train_idx]\n",
    "        val_cases = [train_val_cases[i] for i in val_idx]\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = ACDCDataset(train_cases, phase='train')\n",
    "    val_dataset = ACDCDataset(val_cases, phase='val')    # 从训练集划分的验证集\n",
    "    test_dataset = ACDCDataset(test_cases, phase='val')  # 独立测试集\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    # 模型初始化（保持原有实现不变）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VS_BEAM_3D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5)\n",
    "    \n",
    "    # 初始化跟踪变量\n",
    "    best_acc = 0.0  # 只跟踪最佳准确率\n",
    "    best_loss = 1000\n",
    "    best_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_best.pth\"\n",
    "    final_model_path = f\"{CUSTOM_PREFIX}_fold{fold}_last.pth\"\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)  # 使用新验证集\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # 动态保存最佳模型（只保留最佳准确率版本）\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        \n",
    "        # 早停判断（基于验证损失）\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 10:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    fold_results.append(test_acc)  # 记录测试集准确率\n",
    "    print(f\"Fold {fold+1} Test Accuracy: {test_acc:.2%}\")    \n",
    "    \n",
    "    # 确保最终模型保存（即使早停也保存最后达到的epoch）\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(fold_results, f)\n",
    "    print(f\"\\nCurrent 5-Fold CV Results: {fold_results}\")\n",
    "    print(f\"Average Accuracy: {np.mean(fold_results):.2%} (±{np.std(fold_results):.2%})\")\n",
    "\n",
    "# 输出结果（保持原有实现不变）\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"5-Fold CV Results: {final_results}\")\n",
    "print(f\"Average Accuracy: {np.mean(final_results):.2%} (±{np.std(final_results):.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
